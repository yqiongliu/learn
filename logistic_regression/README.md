<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
##逻辑回归
* [1.起源](#1)<br>
* [2.定义](#2)<br>
* [3.解法](#3)<br>
  *[3.1梯度下降法](#3.1)<br>
  *[3.2牛顿法](#3.1)<br>
  *[3.3拟牛顿法](#3.1)<br>
* [4.正则化](#4)<br>
* [5.和其他算法的关系](#5)<br>
* [6.并行化](#6)<br>

<h2 id="1">1.起源</h2>
（起源是起源，和其完备的定义以及更容易理解的推导无关）

1.1 大意是有人发现了某种现象，于是想用一种公式拟合（描述），<br>
公式描述了事物的变化的某种关系(logistic equation)，进而求解微分方程，可得出事物本身的描述(logistics function )，记为a<br>
对于a，不同的时间有不同的人都给出了其描述公式<br>
![a](https://wikimedia.org/api/rest_v1/media/math/render/svg/6f42e36c949c94189976ae00853af9a1b618e099)

1.2 后来人们又发现，这个描述还可以应用在二分类的概率的场合。<br>
发现如果用`某种形式`描述事件发生的概率，则**事件发生的几率**可以表示为**输入**的**线性组合**，这种`形式`即a<br>
1.3 所谓事件发生的对数几率，其实利用对数几率的定义，可以用一个线性表达式描述一个非线性表达式。<br>
对数几率是一种很好的变换，即`logit = ln(Q/(1-Q))`。


<h2 id="2">2.定义</h2>

数学描述:<br>
2.1逻辑斯蒂分布
(首先还是介绍了一种分布，概率有一种特点，这种特点恰好可用这种分布描述<br>)
logistic function 累计分布函数<br>

$$F(x)=P(X\leq x)=\frac{1}{1 + {e^ {-(x-\mu) / \gamma} }}$$
$$f(x)=F'(x)=\frac{e^ {-(x-\mu)}}{\gamma(1 + {e^ {-(x-\mu) / \gamma })^2 }}$$
2.2逻辑斯蒂回归模型

$$ P(Y=1|x)=\frac{e^{(w\cdot x)}}{1 + e^ {(w\cdot x)}}$$
$$P(Y=0|x)=\frac{1}{1 + e^ {(w\cdot x)}}$$
含义：<br>
通过这种变换，可以用线性拟合非线性关系，即将线性函数表示为概率，线性函数的值接近正无穷，则概率接近1<br>
logistic function，给出了分类概率和输入向量之间的关系，且为决策函数的输出值等于对数几率<br>
而且该函数数学性质好，总是凸的

[补充]
分布函数和回归模型之间的推导

损失函数：<br>
极大似然法估计模型参数
似然函数：<br>
$$\prod_ {i=1}^{N}[P(Y=1|x_{i})]^{y_{i}}[1-P(Y=1|x_{i})]^{1-y_{i}}$$
对数似然函数：<br>
$$L(w)=\sum_ {i=1}^{N}[y_{i}log(P(Y=1|x_{i})+(1-y_{i})log(1-P(Y=1|x_{i}))]$$


含义：<br>

样本是独立同分布的<br>
但事先无需假设数据的具体分布（周志华 机器学习）<br>
特征之间则没有约定是否独立同分布<br>
对于二分类：值只有两种<br>




根据实际的样本值，可以估计出分布中包含的参数。<br>
估计参数可以用最大似然方法，求解最大似然函数需要利用若干迭代的方法<br>

如何解决线性不可分问题？

<h2 id="3">3.解法</h2>

<h3 id="3.1">3.1梯度下降法</h2>

3.1.1公式推导

3.1.2物理意义
泰勒函数的一阶展开
需要利用所有的参数计算->随机梯度算法，批梯度算法
批量梯度下降：在每次更新时用所有样本，要留意，在梯度下降中，对于theta(i)的更新，所有的样本都有贡献，也就是参与theta调整  .其计算得到的是一个标准梯度，也肯定可以达到一个全局最优。因而理论上来说一次更新的幅度是比较大的。
如果样本不多的情况下，当然是这样收敛的速度会更快啦。但是很多时候，样本很多，更新一次要很久，这样的方法就不合适啦。下图是其更新公式

随机梯度下降：在每次更新时用1个样本，可以看到多了随机两个字，随机也就是说我们用样本中的一个例子来近似我所有的样本，来调整θ，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，容易陷入到局部最优解中，但是相比于批量梯度，这样的方法更快，更快收敛，虽然是局部最优，但很多时候是我们可以接受的，所以这个方法用的也比上面的多。下图是其更新公式

mini-batch梯度下降：在每次更新时用b个样本,其实批量的梯度下降就是一种折中的方法，他用了一些小样本来近似全部的，其本质就是我1个指不定不太准，那我用个30个50个样本那比随机的要准不少了吧，而且批量的话还是非常可以反映样本的一个分布情况的。在深度学习中，这种方法用的是最多的，因为这个方法收敛也不会很慢，收敛的局部最优也是更多的可以接受！

梯度下降算法是机器学习或者深度学习中经典算法，现在绝大部分算法都是这个，就算是改进，也是在此基础上做出的改进，所以，要知道这个算法是什么，了解它的思想，甚至推荐直接看论文。

逻辑回归是很多算法的基本
它大致可以分为两个内容，一个是逻辑函数意义，一个是梯度下降法。



梯度下降法是说，求出梯度，并根据步长在梯度方向下降，用矩阵表示反而没有用元素以及求和符号表示更清晰

逻辑回归
如果用线性回归来解释，就是用线性回归模型的预测结果逼近真实标记的对数几率
但是不能直接用均方误差为代价函数
可以考虑用极大似然来估计
3.1.3伪代码

<h3 id="3.2">3.2牛顿法</h2>
3.2.1公式推导

3.2.2物理意义

泰勒函数的二阶展开
需要二阶可导，需要可逆（二者等价？）

<h3 id="3.3">3.3拟牛顿法</h2>

3.3.1公式推导
3.3.2物理意义
3.3.3伪代码
http://blog.csdn.net/itplus/article/details/21897443
牛顿法与拟牛顿法学习笔记（四）BFGS 算法　相应的原本教材　numerical optimization
由于这些伪牛顿法比较复杂，而且在机器学习中也较少用，所以在这里先不细讲了，有兴趣的小伙伴可以看下篇博文
牛顿法在基础机器学习中有用到，但在深度学习中很少用，我们知道是什么，基本了解就好


<h2 id="1">4.正则化</h2>
4.1过拟合
导致过拟合的原因：样本数/特征数<<1，因此解决方法，或者增加样本，或者减少特征，弱化学习能力
样本数不变，减少特征，则方差变大，偏差变小
特征数不变，增加样本，则方差减小，偏差变大
4.2两种方法
4.2.1推导：
4.2.2物理意义：
|x|:
x^2:
4.2.3伪代码：


<h2 id="1">5.和其他算法的关系</h2>
5.1线性回归
从数学上，逻辑回归也可以从线性回归推导得出

线性回归是最简单基本的算法，使用线性函数拟合实际问题，多应用在预测事物发展的趋势。
解线性回归可以用均方误差最小的方法，是以均方误差为代价函数。除此以外还有很多其他的代价函数可以选择。与最小二乘法的关系还待解决。
如果样本很大，或者不具有一些良好的数学性质，可能不能很好的有数学上的完整的形式解，因此考虑最优化的方法，比如梯度下降法
梯度下降法有很多的变种，主要是在计算性能，实时性和结果准确性上寻求最优

首先考虑最简单的线性回归
5.2最大熵
5.3 svm
5.4朴素贝叶斯
5.5能力函数

<h2 id="1">6.并行化</h2>



##参考文献：
http://chenrudan.github.io/blog/2016/01/09/logisticregression.html
【机器学习算法系列之二】浅析Logistic Regression
https://www.zhihu.com/question/20700829
机器学习中使用「正则化来防止过拟合」到底是一个什么原理？为什么正则化项就可以防止过拟合？（推导一下就清楚）
http://blog.csdn.net/u012162613/article/details/48323777 http://www.letiantian.me/2014-10-12-three-models-of-naive-nayes/
贝叶斯与逻辑回归的关系
http://blog.csdn.net/Fishmemory/article/details/51711114
https://www.cnblogs.com/zyber/p/6490663.html
判断模型与生成模型的关系
https://www.cnblogs.com/supersonic/p/3539804.html
先验概率与后验概率
https://www.zhihu.com/question/24094554
最大熵中的特征